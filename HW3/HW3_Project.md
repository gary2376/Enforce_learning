### GPTå•ç­”ç´€éŒ„ï¼šhttps://chatgpt.com/share/67f68e6a-0934-8010-adfe-ae75f35fd5e5

---

## ğŸ° Epsilon-Greedy Algorithm

### (1) æ¼”ç®—æ³•å…¬å¼ï¼ˆLaTeXï¼‰

$$
a_t =
\begin{cases}
\text{random action} & \text{with probability } \varepsilon \\
\arg\max_a Q_t(a) & \text{with probability } 1 - \varepsilon
\end{cases}
$$

$$
Q_{t+1}(a) = Q_t(a) + \alpha (R_t - Q_t(a))
$$

---

### (2) è§£é‡‹è©²ç®—æ³•çš„é—œéµé‚è¼¯æˆ–åˆ†æ

Epsilon-Greedy æ˜¯ä¸€ç¨®å¹³è¡¡ã€Œæ¢ç´¢ã€ï¼ˆexplorationï¼‰èˆ‡ã€Œåˆ©ç”¨ã€ï¼ˆexploitationï¼‰çš„æ–¹æ³•ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œå®ƒä»¥æ©Ÿç‡ $\varepsilon$ éš¨æ©Ÿé¸æ“‡ä¸€å€‹å‹•ä½œï¼ˆæ¢ç´¢ï¼‰ï¼Œä»¥æ©Ÿç‡ $1 - \varepsilon$ é¸æ“‡ç›®å‰ä¼°è¨ˆçå‹µæœ€é«˜çš„å‹•ä½œï¼ˆåˆ©ç”¨ï¼‰ã€‚é€™ç¨®ç­–ç•¥èƒ½é¿å…ç¸½æ˜¯é¸æ“‡å·²çŸ¥æœ€ä½³å‹•ä½œï¼Œå¾è€Œå¯èƒ½ç™¼ç¾æ›´å¥½çš„é¸æ“‡ã€‚  
é¸æ“‡çš„ epsilon å€¼æœƒé¡¯è‘—å½±éŸ¿å­¸ç¿’çš„çµæœï¼Œè¼ƒå¤§çš„ epsilon æœƒåŠ å¼·æ¢ç´¢ã€é¿å…æ—©æœŸéåº¦åˆ©ç”¨ï¼›è¼ƒå°çš„ epsilon æœƒåŠ å¿«æ”¶æ–‚ä½†é¢¨éšªæ˜¯éŒ¯å¤±æ›´å„ªé¸é …ã€‚

---

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨ï¼ˆPython + Matplotlibï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt

class EpsilonGreedy:
    def __init__(self, k_arm=10, epsilon=0.1, steps=1000):
        self.k = k_arm
        self.epsilon = epsilon
        self.steps = steps
        self.q_true = np.random.normal(0, 1, k_arm)
        self.q_est = np.zeros(k_arm)
        self.action_count = np.zeros(k_arm)
        self.rewards = []

    def select_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.k)
        return np.argmax(self.q_est)

    def run(self):
        for _ in range(self.steps):
            action = self.select_action()
            reward = np.random.normal(self.q_true[action], 1)
            self.action_count[action] += 1
            self.q_est[action] += (reward - self.q_est[action]) / self.action_count[action]
            self.rewards.append(reward)
        return self.rewards

# åŸ·è¡Œèˆ‡ç¹ªåœ–
np.random.seed(0)
agent = EpsilonGreedy(epsilon=0.1)
rewards = agent.run()
cumulative_rewards = np.cumsum(rewards)

plt.plot(cumulative_rewards)
plt.title("Epsilon-Greedy: cumulative rewards")
plt.xlabel("steps")
plt.ylabel("accumulated reward")
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/673f61c2-888e-4a02-8832-c9913088b157)

---

### (4) çµæœè§£é‡‹
- æ™‚é–“é¢å‘åˆ†æï¼šEpsilon-Greedy çš„æ”¶æ–‚é€Ÿåº¦å–æ±ºæ–¼ epsilon å€¼ã€‚ç•¶ epsilon è¶¨è¿‘æ–¼ 0 æ™‚ï¼Œç®—æ³•å‚¾å‘æ–¼ exploitationï¼Œä½†å®¹æ˜“é™·å…¥å±€éƒ¨æœ€å„ªï¼›epsilon è¶¨è¿‘æ–¼ 1 æ™‚å‰‡å¹¾ä¹å®Œå…¨æ¢ç´¢ï¼Œå°è‡´æ”¶æ–‚è®Šæ…¢ã€‚
- ç©ºé–“é¢å‘åˆ†æï¼šå„²å­˜ä¼°è¨ˆå€¼èˆ‡è¨ˆæ•¸å™¨æ‰€éœ€ç©ºé–“ç‚º $O(k)$ï¼Œéå¸¸è¼•é‡ï¼Œé©åˆå¤§è¦æ¨¡å•é¡Œã€‚
- æ•ˆæœæ¯”è¼ƒï¼š
  - å„ªé»ï¼šç°¡å–®ã€æ•ˆç‡é«˜ã€å¯¦ç¾å®¹æ˜“ã€‚
  - ç¼ºé»ï¼šå›ºå®šçš„ epsilon ç„¡æ³•æ ¹æ“šå­¸ç¿’é€²åº¦èª¿æ•´æ¢ç´¢ç‡ã€‚

---

## ğŸ“ˆ UCB (Upper Confidence Bound) Algorithm

### (1) æ¼”ç®—æ³•å…¬å¼ï¼ˆLaTeXï¼‰

$$
a_t = \arg\max_a \left[ Q_t(a) + c \cdot \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$

å…¶ä¸­ï¼š
- $Q_t(a)$ï¼šç¬¬ $t$ æ­¥å°å‹•ä½œ $a$ çš„å¹³å‡ä¼°è¨ˆçå‹µ  
- $N_t(a)$ï¼šç¬¬ $t$ æ­¥ä¹‹å‰å‹•ä½œ $a$ è¢«é¸æ“‡çš„æ¬¡æ•¸  
- $c$ï¼šèª¿ç¯€æ¢ç´¢ç¨‹åº¦çš„å¸¸æ•¸  
- $t$ï¼šç›®å‰çš„æ™‚é–“æ­¥

---

### (2) è§£é‡‹è©²ç®—æ³•çš„é—œéµé‚è¼¯æˆ–åˆ†æ

UCB æ¼”ç®—æ³•åŸºæ–¼æ¨‚è§€åˆå§‹åŸå‰‡ï¼ˆOptimism in the Face of Uncertaintyï¼‰ã€‚å®ƒåœ¨é¸æ“‡å‹•ä½œæ™‚ï¼Œé™¤äº†è€ƒæ…®ç•¶å‰çš„å¹³å‡ä¼°è¨ˆçå‹µå¤–ï¼Œé‚„æœƒåŠ ä¸Šä¸€å€‹ã€Œä¸ç¢ºå®šæ€§æ‡²ç½°é …ã€ï¼Œè©²é …æ ¹æ“šå‹•ä½œè¢«é¸æ“‡çš„é »ç‡èª¿æ•´ã€‚  
è‹¥æŸå€‹å‹•ä½œå¾ˆå°‘è¢«é¸æ“‡ï¼Œå…¶ä¸ç¢ºå®šæ€§è¼ƒé«˜ï¼ŒUCB æœƒå‚¾å‘çµ¦äºˆè¼ƒé«˜çš„ã€Œä¿¡è³´ä¸Šé™ã€ï¼Œé¼“å‹µæ¢ç´¢é€™äº›å°šä¸ç¢ºå®šçš„é¸é …ã€‚é€™æ¨£çš„è¨­è¨ˆèƒ½è‡ªç„¶å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ï¼Œä¸éœ€é¡å¤–è¨­å®š epsilonã€‚

---

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨ï¼ˆPython + Matplotlibï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt

class UCB:
    def __init__(self, k_arm=10, c=2, steps=1000):
        self.k = k_arm
        self.c = c
        self.steps = steps
        self.q_true = np.random.normal(0, 1, k_arm)
        self.q_est = np.zeros(k_arm)
        self.action_count = np.zeros(k_arm)
        self.rewards = []

    def select_action(self, t):
        ucb_values = self.q_est + self.c * np.sqrt(np.log(t + 1) / (self.action_count + 1e-5))
        return np.argmax(ucb_values)

    def run(self):
        for t in range(self.steps):
            action = self.select_action(t)
            reward = np.random.normal(self.q_true[action], 1)
            self.action_count[action] += 1
            self.q_est[action] += (reward - self.q_est[action]) / self.action_count[action]
            self.rewards.append(reward)
        return self.rewards

# åŸ·è¡Œèˆ‡ç¹ªåœ–
np.random.seed(0)
agent = UCB(c=2)
rewards = agent.run()
cumulative_rewards = np.cumsum(rewards)

plt.plot(cumulative_rewards)
plt.title("UCB: cumulative rewards")
plt.xlabel("steps")
plt.ylabel("accumulated reward")
plt.grid()
plt.show()

```
![image](https://github.com/user-attachments/assets/3d8831d3-2e90-4c83-a29d-14f82cc25580)

---

### (4) çµæœè§£é‡‹
- æ™‚é–“é¢å‘åˆ†æï¼šUCB æ¼”ç®—æ³•æœƒå„ªå…ˆæ¢ç´¢å°šæœªè¢«é¸æ“‡æˆ–é¸æ“‡æ¬¡æ•¸å°‘çš„å‹•ä½œã€‚é€™ç¨®ç­–ç•¥åœ¨æ—©æœŸæ¢ç´¢å……åˆ†ï¼Œåœ¨å¾ŒæœŸé›†ä¸­åˆ©ç”¨å·²çŸ¥æœ€ä½³é¸é …ï¼Œæ”¶æ–‚é€Ÿåº¦é€šå¸¸æ¯” epsilon-greedy æ›´å¿«ã€‚
- ç©ºé–“é¢å‘åˆ†æï¼šèˆ‡ Epsilon-Greedy ç›¸åŒï¼Œåªéœ€å„²å­˜æ¯å€‹å‹•ä½œçš„ä¼°è¨ˆå€¼èˆ‡æ¬¡æ•¸ï¼Œç©ºé–“è¤‡é›œåº¦ç‚º $O(k)$ã€‚
- æ•ˆæœæ¯”è¼ƒï¼š
  - å„ªé»ï¼šä¸éœ€æ‰‹å‹•è¨­å®šæ¢ç´¢æ©Ÿç‡ï¼Œèƒ½è‡ªå‹•èª¿æ•´æ¢ç´¢å¼·åº¦ã€‚
  - ç¼ºé»ï¼šå° $c$ çš„æ•æ„Ÿåº¦è¼ƒé«˜ï¼Œè‹¥è¨­å®šä¸ç•¶å¯èƒ½å°è‡´éåº¦æ¢ç´¢æˆ–æ”¶æ–‚ä¸ä½³ã€‚

---

## ğŸ”¥ Softmax Algorithm

### (1) æ¼”ç®—æ³•å…¬å¼ï¼ˆLaTeXï¼‰

$$
P(a_t = a) = \frac{e^{Q_t(a)/\tau}}{\sum_{b=1}^{k} e^{Q_t(b)/\tau}}
$$

å…¶ä¸­ï¼š
- $Q_t(a)$ï¼šå‹•ä½œ $a$ åœ¨æ™‚é–“ $t$ çš„ä¼°è¨ˆå€¼
- $\tau$ï¼šæº«åº¦åƒæ•¸ï¼ˆtemperatureï¼‰ï¼Œç”¨ä¾†æ§åˆ¶å‹•ä½œæ©Ÿç‡åˆ†ä½ˆçš„å¹³æ»‘ç¨‹åº¦  
  - $\tau \rightarrow 0$ï¼šæ›´åå‘ exploitation  
  - $\tau \rightarrow \infty$ï¼šæ›´åå‘ uniform æ¢ç´¢

---

### (2) è§£é‡‹è©²ç®—æ³•çš„é—œéµé‚è¼¯æˆ–åˆ†æ

Softmax æ¼”ç®—æ³•ä½¿ç”¨æ©Ÿç‡æ€§é¸æ“‡å‹•ä½œï¼Œæ¯å€‹å‹•ä½œè¢«é¸æ“‡çš„æ©Ÿç‡èˆ‡å…¶ä¼°è¨ˆåƒ¹å€¼å‘ˆæŒ‡æ•¸æ¯”ä¾‹é—œä¿‚ã€‚é€éã€Œæº«åº¦åƒæ•¸ã€$\tau$ ä¾†æ§åˆ¶æ¢ç´¢ç¨‹åº¦ï¼Œç•¶ $\tau$ è¼ƒä½æ™‚æ›´åå¥½é¸æ“‡ä¼°è¨ˆå€¼é«˜çš„å‹•ä½œï¼›$\tau$ è¼ƒé«˜æ™‚å‹•ä½œé¸æ“‡æ›´éš¨æ©Ÿã€‚

èˆ‡ Epsilon-Greedy ä¸åŒçš„æ˜¯ï¼ŒSoftmax ä¸æœƒç¡¬æ€§å°‡æ¢ç´¢èˆ‡åˆ©ç”¨åˆ†é›¢ï¼Œè€Œæ˜¯æ ¹æ“šå„é¸é …çš„åƒ¹å€¼åˆ†ä½ˆåšåŠ æ¬Šé¸æ“‡ï¼Œé€™ä½¿å¾—å®ƒåœ¨è™•ç†ç›¸è¿‘åƒ¹å€¼çš„å‹•ä½œæ™‚æ›´åŠ ç©©å®šèˆ‡é€£è²«ã€‚

---

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨ï¼ˆPython + Matplotlibï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt

class Softmax:
    def __init__(self, k_arm=10, tau=0.1, steps=1000):
        self.k = k_arm
        self.tau = tau
        self.steps = steps
        self.q_true = np.random.normal(0, 1, k_arm)
        self.q_est = np.zeros(k_arm)
        self.action_count = np.zeros(k_arm)
        self.rewards = []

    def softmax_prob(self):
        exp_est = np.exp(self.q_est / self.tau)
        return exp_est / np.sum(exp_est)

    def select_action(self):
        probs = self.softmax_prob()
        return np.random.choice(self.k, p=probs)

    def run(self):
        for _ in range(self.steps):
            action = self.select_action()
            reward = np.random.normal(self.q_true[action], 1)
            self.action_count[action] += 1
            self.q_est[action] += (reward - self.q_est[action]) / self.action_count[action]
            self.rewards.append(reward)
        return self.rewards

# åŸ·è¡Œèˆ‡ç¹ªåœ–
np.random.seed(0)
agent = Softmax(tau=0.1)
rewards = agent.run()
cumulative_rewards = np.cumsum(rewards)

plt.plot(cumulative_rewards)
plt.title("Softmax: ç´¯ç©çå‹µ")
plt.xlabel("æ­¥é©Ÿ")
plt.ylabel("ç´¯ç©çå‹µ")
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/490b699b-edf3-48b2-9aa2-29aa962edf33)

---

### (4) çµæœè§£é‡‹
- æ™‚é–“é¢å‘åˆ†æï¼šSoftmax å¯å¹³æ»‘åœ°åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨ä¹‹é–“åšåˆ‡æ›ï¼Œæ”¶æ–‚é€Ÿåº¦å—åˆ° $\tau$ çš„å¼·çƒˆå½±éŸ¿ã€‚ç•¶ $\tau$ é©ä¸­æ™‚ï¼Œå®ƒèƒ½å¿«é€Ÿè¾¨è­˜ä¸¦é›†ä¸­åœ¨é«˜å›å ±å‹•ä½œï¼›è‹¥ $\tau$ å¤ªé«˜ï¼Œå‰‡æœƒéåº¦æ¢ç´¢è€Œæ‹‰é•·æ”¶æ–‚æ™‚é–“ã€‚
- ç©ºé–“é¢å‘åˆ†æï¼šèˆ‡å…¶ä»–æ¼”ç®—æ³•ç›¸åŒï¼Œæ‰€éœ€å„²å­˜çš„ä¼°è¨ˆå€¼èˆ‡è¨ˆæ•¸å™¨ç‚º $O(k)$ã€‚
- æ•ˆæœæ¯”è¼ƒï¼š
  - å„ªé»ï¼šå…·å‚™é€£çºŒæ€§èˆ‡å¹³æ»‘æ€§çš„æ©Ÿç‡æ§åˆ¶æ–¹å¼ï¼Œé¿å…ç¡¬åˆ‡æ›è¡Œç‚ºï¼Œå°ç›¸è¿‘åƒ¹å€¼çš„å‹•ä½œç‰¹åˆ¥æœ‰æ•ˆã€‚
  - ç¼ºé»ï¼šéœ€è¦é¸æ“‡åˆé©çš„ $\tau$ å€¼ï¼Œä¸”å°æ­¤è¶…åƒæ•¸è¼ƒç‚ºæ•æ„Ÿï¼›è‹¥ $\tau$ å¤ªå°ï¼Œå¯èƒ½è®Šå¾—è¿‘ä¼¼è²ªå©ªç­–ç•¥ã€‚

---

## ğŸ¯ Thompson Sampling Algorithm

### (1) æ¼”ç®—æ³•å…¬å¼ï¼ˆLaTeXï¼‰

å°æ–¼æ¯å€‹å‹•ä½œ $a$ï¼Œç¶­è­·å…¶ Beta åˆ†å¸ƒåƒæ•¸ $(\alpha_a, \beta_a)$ï¼Œæ¯ä¸€æ­¥ï¼š
1. ç‚ºæ¯å€‹å‹•ä½œ $a$ å–æ¨£ï¼š
   $$
   \theta_a \sim \text{Beta}(\alpha_a, \beta_a)
   $$
2. é¸æ“‡æœ€å¤§å€¼å°æ‡‰çš„å‹•ä½œï¼š
   $$
   a_t = \arg\max_a \theta_a
   $$
3. æ ¹æ“šè§€å¯Ÿåˆ°çš„çå‹µ $r_t \in \{0,1\}$ æ›´æ–°åƒæ•¸ï¼š
   $$
   \alpha_a = \alpha_a + r_t,\quad \beta_a = \beta_a + (1 - r_t)
   $$

> å‚™è¨»ï¼šæ­¤è™•æˆ‘å€‘ä»¥ Bernoulli Bandit å•é¡Œï¼ˆçå‹µç‚º 0 æˆ– 1ï¼‰ä½œç‚ºä¾‹å­ã€‚

---

### (2) è§£é‡‹è©²ç®—æ³•çš„é—œéµé‚è¼¯æˆ–åˆ†æ

Thompson Sampling æ˜¯ä¸€ç¨®åŸºæ–¼è²è‘‰æ–¯æ¨è«–çš„ç­–ç•¥ï¼Œå®ƒç‚ºæ¯å€‹å‹•ä½œç¶­è­·ä¸€å€‹æ©Ÿç‡åˆ†å¸ƒï¼ˆé€šå¸¸ç‚º Beta åˆ†å¸ƒï¼‰ï¼Œç”¨ä»¥è¡¨ç¤ºè©²å‹•ä½œç‚ºæœ€å„ªçš„å¯èƒ½æ€§ã€‚  
æ¯ä¸€æ­¥éƒ½å¾é€™äº›åˆ†å¸ƒä¸­å–æ¨£ï¼Œä¸¦é¸æ“‡å–æ¨£çµæœæœ€å¤§çš„å‹•ä½œã€‚é€™æ¨£çš„æ©Ÿåˆ¶åœ¨è‡ªç„¶ä¸­èåˆäº†æ¢ç´¢èˆ‡åˆ©ç”¨â€”â€”å°šæœªè¢«å˜—è©¦éçš„å‹•ä½œæœƒæœ‰è¼ƒå¤§çš„ä¸ç¢ºå®šæ€§ï¼Œå®¹æ˜“è¢«å–æ¨£åˆ°ï¼›åä¹‹ï¼Œè¢«å¤šæ¬¡è§€å¯Ÿå¾Œçš„å‹•ä½œè‹¥è¡¨ç¾ä½³ï¼Œæœƒç©©å®šåœ°è¢«é¸ä¸­ã€‚

é€™ä½¿å¾— Thompson Sampling é€šå¸¸èƒ½å–å¾—å¾ˆå¥½çš„ç´¯ç©çå‹µè¡¨ç¾ï¼Œç‰¹åˆ¥æ˜¯åœ¨ Bernoulli é¡å‹çš„å•é¡Œä¸­ã€‚

---

### (3) ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨ï¼ˆPython + Matplotlibï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt

class ThompsonSampling:
    def __init__(self, k_arm=10, steps=1000):
        self.k = k_arm
        self.steps = steps
        self.q_true = np.random.beta(2, 2, k_arm)  # çœŸå¯¦æ©Ÿç‡ï¼ˆ0~1ï¼‰
        self.alpha = np.ones(k_arm)
        self.beta = np.ones(k_arm)
        self.rewards = []

    def select_action(self):
        theta = np.random.beta(self.alpha, self.beta)
        return np.argmax(theta)

    def run(self):
        for _ in range(self.steps):
            action = self.select_action()
            reward = np.random.binomial(1, self.q_true[action])  # çå‹µç‚º 0 æˆ– 1
            self.alpha[action] += reward
            self.beta[action] += 1 - reward
            self.rewards.append(reward)
        return self.rewards

# åŸ·è¡Œèˆ‡ç¹ªåœ–
np.random.seed(0)
agent = ThompsonSampling()
rewards = agent.run()
cumulative_rewards = np.cumsum(rewards)

plt.plot(cumulative_rewards)
plt.title("Thompson Sampling: ç´¯ç©çå‹µ")
plt.xlabel("æ­¥é©Ÿ")
plt.ylabel("ç´¯ç©çå‹µ")
plt.grid()
plt.show()
```
![image](https://github.com/user-attachments/assets/72ae3bf6-0020-474c-84a3-c9a05c3859f5)

---

### (4) çµæœè§£é‡‹
- æ™‚é–“é¢å‘åˆ†æï¼šThompson Sampling åœ¨åˆæœŸæœƒå¤§é‡æ¢ç´¢ï¼Œä½†éš¨è‘—åƒæ•¸æ”¶æ–‚ï¼Œå®ƒèƒ½å¿«é€Ÿé›†ä¸­æ–¼é«˜å›å ±å‹•ä½œã€‚ç›¸æ¯”å…¶ä»–ç­–ç•¥ï¼Œå®ƒé€šå¸¸æ“æœ‰æ›´å¹³æ»‘èˆ‡ç©©å®šçš„å­¸ç¿’æ›²ç·šã€‚
- ç©ºé–“é¢å‘åˆ†æï¼šéœ€è¦ç‚ºæ¯å€‹å‹•ä½œç¶­è­· $(\alpha, \beta)$ å…©å€‹åƒæ•¸ï¼Œå› æ­¤ç©ºé–“è¤‡é›œåº¦ç‚º $O(k)$ï¼Œèˆ‡å…¶ä»–ç­–ç•¥ç›¸è¿‘ã€‚
- æ•ˆæœæ¯”è¼ƒï¼š
  - å„ªé»ï¼šè‡ªç„¶å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ã€ä¸éœ€äººå·¥èª¿åƒï¼ˆå¦‚ $\varepsilon$ æˆ– $\tau$ï¼‰ï¼Œåœ¨å¤šæ•¸å¯¦é©—ä¸­è¡¨ç¾å„ªç•°ã€‚
  - ç¼ºé»ï¼šéœ€æ ¹æ“šå•é¡Œé¡å‹é¸æ“‡é©ç•¶çš„è²è‘‰æ–¯åˆ†å¸ƒï¼›åœ¨éäºŒå…ƒå›å ±æˆ–æœªçŸ¥åˆ†å¸ƒä¸‹ï¼Œå¯¦ä½œå¯èƒ½è¼ƒç‚ºè¤‡é›œã€‚
 
---

## ğŸ“Š å››ç¨® MAB æ¼”ç®—æ³•çµ±æ•´æ¯”è¼ƒè¡¨

| æ¼”ç®—æ³•             | æ¢ç´¢æ–¹å¼                           | æ˜¯å¦æ©Ÿç‡æ€§é¸æ“‡ | ä¸»è¦è¶…åƒæ•¸         | æ”¶æ–‚é€Ÿåº¦     | å„ªé»                                                       | ç¼ºé»                                                         |
|------------------|----------------------------------|----------------|------------------|------------|----------------------------------------------------------|------------------------------------------------------------|
| Epsilon-Greedy   | æ©Ÿç‡æ€§éš¨æ©Ÿæ¢ç´¢ $\varepsilon$          | å¦              | $\varepsilon$     | ä¸­ç­‰         | å¯¦ä½œç°¡å–®ï¼Œå®¹æ˜“ç†è§£                                          | æ¢ç´¢èˆ‡åˆ©ç”¨æ˜¯ç¡¬åˆ‡åˆ†ï¼Œ $\varepsilon$ å›ºå®šå¯èƒ½ä¸éˆæ´»                 |
| UCB              | åŸºæ–¼ç½®ä¿¡å€é–“ï¼Œè‡ªå‹•å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨          | å¦              | $c$ï¼ˆæ¢ç´¢å¼·åº¦ï¼‰     | å¿«           | ç„¡éœ€é¡å¤–æ¢ç´¢æ©Ÿç‡ï¼Œè‡ªå‹•èª¿æ•´æ¢ç´¢ç¨‹åº¦                              | å° $c$ å€¼æ•æ„Ÿï¼Œè¨­å®šä¸ç•¶å¯èƒ½å°è‡´éåº¦æ¢ç´¢æˆ–åˆ©ç”¨                        |
| Softmax          | æ ¹æ“šå‹•ä½œä¼°å€¼ä»¥ softmax æ©Ÿç‡é¸æ“‡         | æ˜¯              | $\tau$ï¼ˆæº«åº¦åƒæ•¸ï¼‰ | ä¸­ç­‰         | å¹³æ»‘æ§åˆ¶æ¢ç´¢å¼·åº¦ï¼Œå°ä¼°å€¼æ¥è¿‘çš„å‹•ä½œè¡¨ç¾æ›´ç©©å®š                        | å° $\tau$ æ•æ„Ÿï¼Œè‹¥è¨­å®šå¤ªå°å‰‡è¿‘ä¼¼è²ªå©ªç­–ç•¥ï¼›å¤ªå¤§å‰‡éåº¦æ¢ç´¢              |
| Thompson Sampling | å¾æ¯å€‹å‹•ä½œçš„å¾Œé©—åˆ†å¸ƒä¸­å–æ¨£              | æ˜¯              | åˆå§‹å…ˆé©—åƒæ•¸ï¼ˆå¦‚ $\alpha, \beta$ï¼‰ | å¿«           | è‡ªç„¶èåˆæ¢ç´¢èˆ‡åˆ©ç”¨ã€è¡¨ç¾ç©©å®šã€ä¸éœ€æ˜ç¢ºæ¢ç´¢æ©Ÿç‡                       | å¯¦ä½œéœ€æ ¹æ“šå•é¡Œè¨­è¨ˆåˆé©çš„è²è‘‰æ–¯æ¨¡å‹ï¼Œé 0/1 çå‹µæ™‚æœƒè¼ƒè¤‡é›œ              |

---

âœ… **çµè«–å»ºè­°**ï¼š
- è‹¥æƒ³å¿«é€Ÿä¸Šæ‰‹ä¸”æ§åˆ¶ç°¡å–®ï¼Œå¯å¾ **Epsilon-Greedy** é–‹å§‹ã€‚
- è‹¥éœ€è¦ç©©å®šè¡¨ç¾èˆ‡è‡ªå‹•åŒ–æ¢ç´¢ç­–ç•¥ï¼Œ**UCB** èˆ‡ **Thompson Sampling** è¡¨ç¾é€šå¸¸è¼ƒä½³ã€‚
- è‹¥å¸Œæœ›æ©Ÿç‡æ€§æ¢ç´¢ï¼Œä¸”æœ‰é€£çºŒæ§åˆ¶éœ€æ±‚ï¼Œå¯è€ƒæ…® **Softmax**ã€‚

---



