
# ğŸ§  HW4-1: éœæ…‹æ¨¡å¼çš„æ¨¸ç´  DQN ç†è§£å ±å‘Š

## âœ… ä½œæ¥­ç›®æ¨™èˆ‡å…§å®¹èªªæ˜

æœ¬æ¬¡ä½œæ¥­ç›®æ¨™ç‚ºç†è§£ä¸¦å¯¦ä½œ Deep Q-Networkï¼ˆDQNï¼‰æ–¼éœæ…‹æ¨¡å¼ä¸‹çš„æ‡‰ç”¨ï¼ŒæŒæ¡å¼·åŒ–å­¸ç¿’æ ¸å¿ƒæµç¨‹ï¼Œä¸¦å»ºç«‹å° Replay Bufferã€Epsilon-Greedy ç­–ç•¥ã€Q-value æ›´æ–°æ©Ÿåˆ¶çš„å¯¦ä½œç†è§£ã€‚

---

## ğŸ“š ç†è«–èƒŒæ™¯èˆ‡ DQN èµ·æº

### å¼·åŒ–å­¸ç¿’ç°¡ä»‹

å¼·åŒ–å­¸ç¿’ï¼ˆReinforcement Learning, RLï¼‰æ˜¯ä¸€ç¨®åŸºæ–¼å›é¥‹å­¸ç¿’çš„æ±ºç­–æ–¹å¼ï¼Œè®“æ™ºèƒ½é«”ï¼ˆAgentï¼‰é€éèˆ‡ç’°å¢ƒäº’å‹•ç²å–çå‹µï¼ˆRewardï¼‰ï¼Œé€²è€Œå­¸ç¿’æœ€é©ç­–ç•¥ï¼ˆPolicyï¼‰ã€‚å‚³çµ±æ–¹æ³•å¦‚ Q-Learning éœ€ç¶­è­·ä¸€å¼µ Q-tableï¼Œç•¶ç‹€æ…‹ç©ºé–“è®Šå¤§æ™‚å°‡è®Šå¾—ä¸å¯è¡Œã€‚

### æ·±åº¦ Q ç¶²è·¯ï¼ˆDeep Q-Network, DQNï¼‰

DQN æ˜¯ DeepMind åœ˜éšŠæ–¼ 2015 å¹´æå‡ºçš„æ–¹æ³•ï¼Œé€éç¥ç¶“ç¶²è·¯é€¼è¿‘ Q å‡½æ•¸ï¼Œä½¿å¾— Q-Learning å¯ä»¥è™•ç†é«˜ç¶­åº¦ç‹€æ…‹ç©ºé–“ï¼Œé¦–æ¬¡å¯¦ç¾ç”¨å–®ä¸€æ¶æ§‹ç©å¤šå€‹ Atari éŠæˆ²ä¸¦é”äººé¡æ°´æº–ã€‚å…¶é—œéµå‰µæ–°åŒ…æ‹¬ï¼š
1. **Experience Replay**
2. **Target Network**
3. **æ¢¯åº¦ä¸‹é™æ›´æ–° Q ç¶²è·¯**

---

## ğŸ”§ DQN æ¶æ§‹èˆ‡æ ¸å¿ƒæ¨¡çµ„å¯¦ä½œèªªæ˜

### 1. Q-Network å»ºæ§‹

```python
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=64):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```
- ä½¿ç”¨ä¸‰å±¤å…¨é€£æ¥ç¥ç¶“ç¶²è·¯ï¼Œé©ç”¨æ–¼ä½ç¶­ç‹€æ…‹ç©ºé–“ã€‚
- ReLU æ¿€æ´»å‡½æ•¸èƒ½æœ‰æ•ˆé¿å…æ¢¯åº¦æ¶ˆå¤±ã€‚

### 2. æå¤±è¨ˆç®—èˆ‡ Q æ›´æ–°

```python
q_target = reward + gamma * torch.max(q_next, dim=1)[0] * (1 - done)
q_eval = q_net(state).gather(1, action.unsqueeze(1)).squeeze()
loss = F.mse_loss(q_eval, q_target.detach())
```

- è‹¥ `done` ç‚º Trueï¼Œå‰‡ä¸è€ƒæ…®ä¸‹ä¸€ç‹€æ…‹çš„ Q å€¼ã€‚
- ä½¿ç”¨ `.detach()` é¿å… target value å½±éŸ¿æ¢¯åº¦æ›´æ–°ã€‚

### 3. Epsilon-Greedy ç­–ç•¥æ¢ç´¢

```python
if random.random() < epsilon:
    action = env.action_space.sample()
else:
    with torch.no_grad():
        action = q_net(state).argmax().item()
```

> åˆæœŸé«˜æ¢ç´¢ï¼Œé€æ­¥è¡°æ¸› epsilon å¢åŠ åˆ©ç”¨ç‡ã€‚

---

## â™»ï¸ Replay Buffer ç·©è¡å€è¨­è¨ˆ

```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
```

- ç¶“é©—æ¨£æœ¬ä»¥äº”å…ƒçµ„ (s, a, r, s', done) å„²å­˜ã€‚
- æŠ½æ¨£æ™‚æ‰“äº‚é †åºï¼Œæ‰“ç ´è³‡æ–™ç›¸é—œæ€§ï¼Œæå‡å­¸ç¿’ç©©å®šæ€§ã€‚

---

## ğŸ§ª å¯¦é©—æµç¨‹èˆ‡è¨­å®š

### è¨“ç·´æµç¨‹ Step-by-Step

1. åˆå§‹åŒ– Q ç¶²è·¯èˆ‡ Replay Buffer
2. é‡è¤‡ä»¥ä¸‹æ­¥é©Ÿç›´åˆ°å®Œæˆè¨“ç·´
   - å–å¾—ç•¶å‰ç‹€æ…‹ä¸¦é¸æ“‡å‹•ä½œï¼ˆä¾ epsilon-greedyï¼‰
   - åŸ·è¡Œå‹•ä½œä¸¦ç²å–ç’°å¢ƒå›é¥‹
   - å„²å­˜äº”å…ƒçµ„è‡³ Replay Buffer
   - éš¨æ©ŸæŠ½æ¨£ç¶“é©—ä¸¦è¨“ç·´ Q ç¶²è·¯
   - éæ¸› epsilonï¼ˆæ¢ç´¢ç‡ï¼‰

### è¶…åƒæ•¸è¨­å®šåƒè€ƒ

| åƒæ•¸         | èªªæ˜                        | å»ºè­°å€¼æˆ–ç¯„åœ       |
|--------------|-----------------------------|--------------------|
| learning rate| å­¸ç¿’ç‡                      | 1e-3 ~ 1e-4         |
| gamma        | æŠ˜æ‰£å› å­                    | 0.95 ~ 0.99         |
| epsilon      | æ¢ç´¢ç‡åˆå§‹å€¼                | 1.0ï¼Œç·šæ€§è¡°æ¸›åˆ° 0.1 |
| buffer size  | Replay Buffer å¤§å°          | 10000 ~ 50000       |
| batch size   | æ¯æ¬¡è¨“ç·´æŠ½æ¨£æ•¸é‡            | 32 æˆ– 64            |

---

## ğŸ§© æ”¹é€²æ–¹å‘èˆ‡å¸¸è¦‹å•é¡Œ

| å•é¡Œ | è§£æ³• |
|------|------|
| Q å€¼æŒ¯ç›ªä¸ç©© | å¼•å…¥ Target Network |
| Q å€¼é«˜ä¼° | ä½¿ç”¨ Double DQN |
| æ”¶æ–‚é€Ÿåº¦æ…¢ | åŠ å¿« epsilon è¡°æ¸›ã€ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦å™¨ |
| æ¢ç´¢æ•ˆæœä¸ä½³ | æ¡ç”¨ Boltzmann ç­–ç•¥æˆ– Noisy Net |
| æ¨£æœ¬åˆ©ç”¨ç‡ä½ | ä½¿ç”¨ Prioritized Experience Replay |

---

## ğŸ’¬ èˆ‡ ChatGPT è¨è«–é‡é»æ‘˜è¦

- æ·±å…¥æ¢è¨ `.detach()` çš„ä½œç”¨èˆ‡ Backpropagation è¡Œç‚ºã€‚
- å° Replay Buffer è¨˜æ†¶é«”çµæ§‹èˆ‡ sample è¡Œç‚ºé€²è¡Œ traceã€‚
- æ¸¬è©¦ä¸åŒ batch size å°å­¸ç¿’ç©©å®šæ€§çš„å½±éŸ¿ã€‚
- å»ºè­°åŠ å…¥ reward shaping æ”¹å–„ç­–ç•¥åå·®å•é¡Œã€‚

---

## ğŸ“Š å¯è£œä¸Šåœ–è¡¨èˆ‡åˆ†æï¼ˆé ç•™ä½ç½®ï¼‰

- ğŸ“ˆ **å­¸ç¿’æ›²ç·š**ï¼ˆepisode reward å° episode æ•¸ï¼‰
- ğŸ“‰ **epsilon è¡°æ¸›è¦–è¦ºåŒ–**
- ğŸ¯ **æˆåŠŸç‡è®ŠåŒ–è¶¨å‹¢**

---

## ğŸ“ ç¸½çµ

æœ¬ä½œæ¥­é€ééœæ…‹æ¨¡å¼çš„ DQN å¯¦ä½œï¼Œå¾åº•å±¤ç†è§£å¼·åŒ–å­¸ç¿’å„çµ„ä»¶ä¹‹é–“çš„é—œè¯èˆ‡è¨­è¨ˆç›®çš„ã€‚Replay Buffer çš„è¨­è¨ˆå¤§å¹…åº¦æå‡æ¨£æœ¬æ•ˆç‡èˆ‡å­¸ç¿’ç©©å®šæ€§ï¼Œè€Œ epsilon-greedy ç­–ç•¥å±•ç¾äº†æ¢ç´¢èˆ‡åˆ©ç”¨çš„æ¬Šè¡¡å“²å­¸ã€‚è‹¥æœªä¾†é€²ä¸€æ­¥å°å…¥ Target Networkã€Double DQNã€Dueling DQN ç­‰æŠ€å·§ï¼Œé æœŸèƒ½é¡¯è‘—æ”¹å–„ç­–ç•¥è¡¨ç¾èˆ‡å­¸ç¿’é€Ÿåº¦ã€‚



---


# ğŸ§  HW4-1: éœæ…‹æ¨¡å¼çš„æ¨¸ç´  DQN ç†è§£å ±å‘Šï¼ˆå«è©³ç´°ç¨‹å¼ç¢¼èªªæ˜ï¼‰

## âœ… ä½œæ¥­ç›®æ¨™èˆ‡å…§å®¹èªªæ˜

æœ¬ä½œæ¥­è¦æ±‚å­¸ç”Ÿäº†è§£ä¸¦å¯¦ä½œ DQNï¼ˆDeep Q-Networkï¼‰æ¼”ç®—æ³•ï¼Œä¸¦é€ééœæ…‹æ¨¡å¼å­¸ç¿’å…¶æ ¸å¿ƒæµç¨‹èˆ‡å¯¦ä½œç´°ç¯€ã€‚æœ¬å ±å‘Šå°‡ä»¥ç¨‹å¼ç¢¼ç‚ºä¸»è»¸ï¼Œé€æ®µèªªæ˜å„æ¨¡çµ„é‚è¼¯èˆ‡å…¶å°å­¸ç¿’çš„è²¢ç»ã€‚

---

## ğŸ§  1. å»ºç«‹ Q-Network æ¨¡å‹

```python
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=64):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
```

### æ¨¡å‹æ¶æ§‹èªªæ˜ï¼š
- `state_size`ï¼šè¼¸å…¥ç‹€æ…‹ç©ºé–“ç¶­åº¦ï¼Œä¾‹å¦‚ (4,) è¡¨ç¤ºç’°å¢ƒç‹€æ…‹å‘é‡é•·åº¦ç‚º 4ã€‚
- `action_size`ï¼šå‹•ä½œç©ºé–“å€‹æ•¸ï¼Œä¾‹å¦‚åœ¨ CartPole ä¸­ç‚º 2ï¼ˆå·¦ç§»æˆ–å³ç§»ï¼‰ã€‚
- `hidden_size`ï¼šéš±è—å±¤ç¥ç¶“å…ƒå€‹æ•¸ï¼Œé€šå¸¸è¨­å®šç‚º 64ã€128 æˆ– 256ã€‚
- `fc1 ~ fc3`ï¼šä¸‰å±¤å…¨é€£æ¥å±¤ï¼ˆLinear Layerï¼‰ï¼Œæœ€çµ‚è¼¸å‡ºå°æ¯å€‹å‹•ä½œçš„ Q å€¼ä¼°è¨ˆã€‚

```python
    def forward(self, x):
        x = F.relu(self.fc1(x))  # ç¬¬ä¸€å±¤è¼¸å…¥å¾Œé€²è¡Œ ReLU æ¿€æ´»
        x = F.relu(self.fc2(x))  # ç¬¬äºŒå±¤ä¹Ÿä½¿ç”¨ ReLU
        return self.fc3(x)       # æœ€çµ‚è¼¸å‡ºå°æ‡‰å„å€‹å‹•ä½œçš„ Q å€¼å‘é‡
```

> ReLU æ¿€æ´»æœ‰åŠ©æ–¼é¿å…æ¢¯åº¦æ¶ˆå¤±å•é¡Œï¼Œä½¿ç¶²è·¯èƒ½æ›´æœ‰æ•ˆå­¸ç¿’éç·šæ€§é—œä¿‚ã€‚

---

## ğŸ’¾ 2. ç¶“é©—é‡æ’­ç·©è¡å€ï¼ˆReplay Bufferï¼‰

```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)
```

- ä½¿ç”¨ deque å¯¦ä½œ FIFO ç·©è¡å€ï¼Œ`maxlen=capacity` é™åˆ¶æœ€å¤§å„²å­˜æ•¸é‡ï¼Œè¶…éæ™‚è‡ªå‹•ç§»é™¤æœ€èˆŠè³‡æ–™ã€‚

```python
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
```

- æ¯æ¬¡èˆ‡ç’°å¢ƒäº’å‹•å¾Œï¼Œå°‡ç¶“é©—äº”å…ƒçµ„è¨˜éŒ„ä¸‹ä¾†ã€‚

```python
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
```

- éš¨æ©ŸæŠ½å–æ¨£æœ¬å¯æ‰“ç ´æ™‚é–“åºåˆ—ä¾è³´ï¼Œæœ‰åŠ©æ–¼è¨“ç·´ç©©å®šã€‚
- `zip(*)` è§£æ§‹ batch ç‚ºå¤šå€‹ arrayï¼Œ`np.stack` å°‡å®ƒå€‘è½‰ç‚ºæ‰¹æ¬¡è¼¸å…¥æ ¼å¼ã€‚

---

## ğŸ¯ 3. å‹•ä½œé¸æ“‡ï¼ˆEpsilon-Greedyï¼‰

```python
if random.random() < epsilon:
    action = env.action_space.sample()
else:
    with torch.no_grad():
        action = q_net(state).argmax().item()
```

- ç•¶éš¨æ©Ÿå€¼ < epsilon æ™‚ï¼Œé¸æ“‡éš¨æ©Ÿå‹•ä½œï¼ˆæ¢ç´¢ï¼‰ã€‚å¦å‰‡ä½¿ç”¨ Q ç¶²è·¯é¸æ“‡æœ€å¤§å€¼å‹•ä½œï¼ˆåˆ©ç”¨ï¼‰ã€‚
- ä½¿ç”¨ `with torch.no_grad()` å¯é¿å…è¨ˆç®—åœ–æ§‹å»ºï¼ŒåŠ å¿«æ¨è«–é€Ÿåº¦ã€‚

> è¡°æ¸› epsilon å¯å¯¦ç¾å¾ã€Œå¤šæ¢ç´¢ã€æ¼¸é€²è½‰å‘ã€Œå¤šåˆ©ç”¨ã€ï¼Œæ˜¯ä¸€ç¨®å…¸å‹çš„ exploration-exploitation å¹³è¡¡æ–¹å¼ã€‚

---

## ğŸ” 4. Q å€¼æ›´æ–°æµç¨‹

```python
q_next = target_net(next_state_tensor).detach()
q_target = reward_tensor + gamma * q_next.max(1)[0] * (1 - done_tensor)
q_eval = q_net(state_tensor).gather(1, action_tensor.unsqueeze(1)).squeeze()
loss = F.mse_loss(q_eval, q_target)
```

- `q_next.max(1)[0]`ï¼šä»£è¡¨ä¸‹å€‹ç‹€æ…‹ä¸­æ‰€æœ‰å‹•ä½œçš„æœ€å¤§ Q å€¼ï¼ˆå°æ‡‰ Bellman æœ€ä½³å€¼ä¼°è¨ˆï¼‰ã€‚
- `detach()`ï¼šé¿å… target Q å€¼åƒèˆ‡æ¢¯åº¦åå‚³ã€‚
- `gather(1, action)`ï¼šæå–ç•¶å‰æ‰€é¸å‹•ä½œå°æ‡‰çš„ Q å€¼ã€‚

---

## ğŸ‹ï¸â€â™‚ï¸ 5. è¨“ç·´èˆ‡åƒæ•¸æ›´æ–°

```python
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

- `zero_grad()`ï¼šæ¯å€‹ batch è¨“ç·´å‰é‡ç½®æ¢¯åº¦ã€‚
- `loss.backward()`ï¼šåå‘å‚³æ’­èª¤å·®ã€‚
- `optimizer.step()`ï¼šæ ¹æ“šæ¢¯åº¦æ›´æ–°åƒæ•¸ã€‚

> æœ€å¸¸ç”¨çš„å„ªåŒ–å™¨ç‚º Adamï¼Œå…·æœ‰è‡ªå‹•èª¿æ•´å­¸ç¿’ç‡èˆ‡æ¢¯åº¦è¦ç¯„çš„å„ªé»ã€‚

---

## âœ… å»ºè­°è£œå……çš„è¨“ç·´ä¸»è¿´åœˆæ¡†æ¶

```python
for episode in range(MAX_EPISODES):
    state = env.reset()
    total_reward = 0
    for step in range(MAX_STEPS):
        action = select_action(state, epsilon)
        next_state, reward, done, _ = env.step(action)
        buffer.push(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward

        if len(buffer) >= BATCH_SIZE:
            update_q_network(buffer.sample(BATCH_SIZE))
        
        if done:
            break
    epsilon = max(min_epsilon, epsilon * epsilon_decay)
```

- `MAX_EPISODES`ï¼šæœ€å¤§è¨“ç·´å›åˆæ•¸
- `total_reward` å¯è¨˜éŒ„ç•¶å‰ episode æˆæœ
- å¯ä¾ç…§ episode plot å‡ºå­¸ç¿’æ›²ç·š

---

## ğŸ“˜ ç¸½çµï¼šDQN å„æ¨¡çµ„çš„å”ä½œé—œä¿‚

- `QNetwork` æ˜¯æ ¸å¿ƒä¼°è¨ˆå™¨ï¼ŒæŒçºŒå­¸ç¿’ Q å€¼å‡½æ•¸ã€‚
- `ReplayBuffer` ä¿ç•™å¤§é‡éå¾€ç¶“é©—ä»¥ç©©å®šè¨“ç·´ã€‚
- `Epsilon-Greedy` ç‚ºç­–ç•¥å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨ã€‚
- æå¤±è¨ˆç®—æ ¹æ“š Bellman equation æŒ‡å°å­¸ç¿’æ–¹å‘ã€‚
- ä½¿ç”¨ `detach()`ã€`optimizer` ç­‰ PyTorch ç‰¹æ€§ä½¿æ¨¡å‹è¨“ç·´æ›´åŠ ç©©å®šèˆ‡é«˜æ•ˆã€‚

ä»¥ä¸Šèªªæ˜ä¸åƒ…æœ‰åŠ©æ–¼ç†è§£éœæ…‹ DQN å¯¦ä½œï¼Œæ›´ç‚ºé€²ä¸€æ­¥å°å…¥æ”¹é€²æ¶æ§‹ï¼ˆå¦‚ Duelingã€Prioritizedã€Double DQNï¼‰æ‰“ä¸‹åŸºç¤ã€‚
