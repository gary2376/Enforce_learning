# ğŸ’¡ å¦‚ä½•æ”¹é€²åŸºæœ¬çš„ DQN æ–¹æ³•ï¼šDouble DQN èˆ‡ Dueling DQN

å¼·åŒ–å­¸ç¿’ä¸­ï¼ŒDQNï¼ˆDeep Q-Networkï¼‰çµåˆäº†æ·±åº¦ç¥ç¶“ç¶²è·¯èˆ‡ Q-learningï¼Œè®“æ™ºèƒ½é«”èƒ½åœ¨é«˜ç¶­åº¦ç‹€æ…‹ç©ºé–“ä¸­é€²è¡Œæ±ºç­–ã€‚ç„¶è€Œï¼ŒåŸºæœ¬ DQN ä»æœ‰å…©å¤§ç¼ºé™·ï¼š

---

## â— åŸºæœ¬ DQN çš„å•é¡Œ

1. **Q å€¼é«˜ä¼°ï¼ˆOverestimation Biasï¼‰**åŸºæœ¬ DQN åŒæ™‚ç”¨åŒä¸€å€‹ç¶²è·¯æ±ºå®šæœ€å¤§ Q å€¼çš„å‹•ä½œå’Œä¼°è¨ˆè©²å‹•ä½œçš„ Q å€¼ï¼Œå®¹æ˜“å°è‡´éŒ¯èª¤æ”¾å¤§ï¼Œå­¸ç¿’ä¸ç©©å®šã€‚
2. **ç„¡æ³•æœ‰æ•ˆå»ºæ¨¡ç‹€æ…‹åƒ¹å€¼ï¼ˆInefficient Learningï¼‰**
   åœ¨æŸäº›ç‹€æ…‹ä¸‹ï¼Œé¸æ“‡å“ªå€‹å‹•ä½œå…¶å¯¦æ²’å·®ï¼Œä½† DQN å°æ¯å€‹ `(s, a)` é…å°éƒ½å–®ç¨å­¸ç¿’ï¼Œæ•ˆç‡ä½ä¸‹ã€‚

---

## ğŸ§  æ”¹é€²ä¸€ï¼šDouble DQN

### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

å°‡ã€Œé¸æ“‡å‹•ä½œã€èˆ‡ã€Œä¼°è¨ˆ Q å€¼ã€åˆ†é–‹ï¼š

- **Online Network**ï¼šé¸æ“‡æœ€ä½³å‹•ä½œï¼ˆ`argmax`ï¼‰
- **Target Network**ï¼šè©•ä¼°è©²å‹•ä½œçš„ Q å€¼

é€™æ¨£å¯é™ä½é«˜ä¼°åå·®ï¼Œæå‡æ”¶æ–‚ç©©å®šæ€§ã€‚

### ğŸ“Œ Q å€¼æ›´æ–°å…¬å¼ï¼š

```
Q(s, a) = r + Î³ * Q_target(s', argmax_a' Q_online(s', a'))
```

### ğŸ” ç¨‹å¼ç¢¼èªªæ˜ç‰‡æ®µ

```python
with torch.no_grad():
    q_online = model(state2_batch)           # Online ç¶²è·¯ï¼šæ±ºå®šå‹•ä½œ
    q_target = target_model(state2_batch)    # Target ç¶²è·¯ï¼šä¼°è¨ˆè©²å‹•ä½œçš„ Q å€¼
    next_actions = q_online.argmax(dim=1, keepdim=True)
    Q2 = q_target.gather(1, next_actions).squeeze()
```

é€™æ®µä¸­ï¼Œ`model` ç”¨ä¾†é¸å‡ºæœ€å¤§å€¼å‹•ä½œç´¢å¼•ï¼Œè€Œ `target_model` å°é€™äº›å‹•ä½œä¼°ç®— Q å€¼ã€‚

---

## ğŸ§  æ”¹é€²äºŒï¼šDueling DQN

### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

æ‹†è§£ Q å€¼ç‚ºå…©éƒ¨åˆ†ï¼š

- **V(s)**ï¼šç‹€æ…‹æœ¬èº«çš„åƒ¹å€¼ï¼ˆä¸ä¾è³´å‹•ä½œï¼‰
- **A(s,a)**ï¼šå‹•ä½œçš„ç›¸å°å„ªå‹¢ï¼ˆä»£è¡¨æ­¤å‹•ä½œå¥½å£ï¼‰

### ğŸ“Œ åˆæˆå…¬å¼ï¼š

```
Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))
```

### ğŸ” æ¨¡å‹æ¶æ§‹ç¨‹å¼ç¢¼ï¼š

```python
class DuelingQNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.feature = nn.Sequential(
            nn.Linear(64, 150), nn.ReLU()
        )
        self.value_stream = nn.Sequential(
            nn.Linear(150, 100), nn.ReLU(), nn.Linear(100, 1)
        )
        self.advantage_stream = nn.Sequential(
            nn.Linear(150, 100), nn.ReLU(), nn.Linear(100, 4)
        )

    def forward(self, x):
        x = self.feature(x)
        v = self.value_stream(x)
        a = self.advantage_stream(x)
        return v + a - a.mean(dim=1, keepdim=True)
```

Dueling æ¶æ§‹é©åˆã€ŒæŸäº›å‹•ä½œå¹¾ä¹ç„¡å·®ç•°ã€çš„ç’°å¢ƒï¼Œå¦‚åœ¨ç‰†è§’ç«™è‘—çš„å ´æ™¯ï¼Œèƒ½æå‡è¨“ç·´æ•ˆç‡ã€‚

---

## ğŸ“Š æ–¹æ³•æ¯”è¼ƒ

| æ–¹æ³•        | æ”¹é€²é‡é»               | è§£æ±ºå•é¡Œ             | æ˜¯å¦å¢åŠ ç¶²è·¯è¤‡é›œåº¦ |
| ----------- | ---------------------- | -------------------- | ------------------ |
| Double DQN  | åˆ†é›¢é¸æ“‡èˆ‡ä¼°å€¼         | Q å€¼é«˜ä¼°             | å¦ï¼ˆéœ€é›™ç¶²è·¯ï¼‰     |
| Dueling DQN | æ‹†åˆ†ç‹€æ…‹åƒ¹å€¼èˆ‡å‹•ä½œå„ªå‹¢ | å­¸ç¿’æ•ˆç‡å·®ã€å‹•ä½œå†—é¤˜ | æ˜¯ï¼ˆæ”¹æ¶æ§‹ï¼‰       |

---

## âœ… ç¸½çµ

é€é Double DQN èˆ‡ Dueling DQNï¼Œèƒ½å¾ã€ŒQ å€¼ä¼°è¨ˆèª¤å·®ã€èˆ‡ã€Œå­¸ç¿’æ•ˆç‡ã€å…©æ–¹é¢æå‡åŸºæœ¬ DQNï¼Œè®“å¼·åŒ–å­¸ç¿’æ¨¡å‹åœ¨è¨“ç·´ç©©å®šæ€§èˆ‡æ³›åŒ–èƒ½åŠ›ä¸Šæ›´ä¸Šä¸€å±¤æ¨“ã€‚

---

## ğŸ§ª Double DQN ç¨‹å¼ç¢¼èªªæ˜

ä»¥ä¸‹ç‚º Double DQN åœ¨ GridWorld ç’°å¢ƒä¸­çš„å¯¦ä½œé—œéµï¼š

```python
class QNetwork(nn.Module):
    def __init__(self):
        ...
        self.fc1 = nn.Linear(64, 150)
        self.fc2 = nn.Linear(150, 100)
        self.fc3 = nn.Linear(100, 4)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

- ä½¿ç”¨å–®ä¸€ Q å€¼è¼¸å‡ºï¼Œæ¯å€‹å‹•ä½œå°æ‡‰ä¸€å€‹è¼¸å‡ºç¶­åº¦ã€‚
- Replay Buffer å„²å­˜ç¶“é©—ï¼Œä¸¦ä½¿ç”¨ `model` åšç­–ç•¥é¸æ“‡ï¼Œ`target_model` åš Q å€¼è©•ä¼°ã€‚

æ ¸å¿ƒæ›´æ–°æ®µè½ï¼š

```python
with torch.no_grad():
    online = model(s2)
    target = target_model(s2)
    a_prime = online.argmax(dim=1, keepdim=True)
    q2 = target.gather(1, a_prime).squeeze()
```

é€™è£¡å°±æ˜¯ **Double DQN çš„æ ¸å¿ƒæ©Ÿåˆ¶**ï¼šç”¨ `online` é¸å‹•ä½œï¼Œç”¨ `target` è©•ä¼°è©²å‹•ä½œçš„åƒ¹å€¼ã€‚

---

## ğŸ§ª Dueling DQN ç¨‹å¼ç¢¼èªªæ˜

ä»¥ä¸‹ç‚º Dueling DQN çš„ PyTorch æ¨¡å‹ï¼š

```python
class DuelingQNetwork(nn.Module):
    def __init__(self):
        ...
        self.feature = nn.Sequential(nn.Linear(64, 150), nn.ReLU())
        self.value_stream = nn.Sequential(nn.Linear(150, 100), nn.ReLU(), nn.Linear(100, 1))
        self.advantage_stream = nn.Sequential(nn.Linear(150, 100), nn.ReLU(), nn.Linear(100, 4))

    def forward(self, x):
        x = self.feature(x)
        v = self.value_stream(x)
        a = self.advantage_stream(x)
        return v + a - a.mean(dim=1, keepdim=True)
```

- `feature` å±¤è² è²¬æå–ç‹€æ…‹ç‰¹å¾µã€‚
- `value_stream` è¼¸å‡ºç‹€æ…‹åƒ¹å€¼ V(s)ã€‚
- `advantage_stream` è¼¸å‡ºå‹•ä½œå„ªå‹¢ A(s, a)ã€‚
- æœ€çµ‚åˆä½µç‚º Q å€¼ã€‚

Q å€¼è¨ˆç®—é‚è¼¯ï¼š

```python
return v + a - a.mean(dim=1, keepdim=True)
```

é€™ç¨®æ¶æ§‹æœ‰åŠ©æ–¼åœ¨æŸäº›å‹•ä½œå·®ç•°ä¸å¤§çš„ç‹€æ…‹ä¸­ä»èƒ½æœ‰æ•ˆå­¸ç¿’ç‹€æ…‹åƒ¹å€¼ï¼Œæå‡æ”¶æ–‚é€Ÿåº¦ã€‚

---

## ğŸ§ª è¦–è¦ºåŒ–çµæœ

è¨“ç·´çµæŸå¾Œï¼Œå…©å€‹æ¨¡å‹çš†ç¹ªè£½ loss æ”¶æ–‚æ›²ç·šï¼š

```python
plt.plot(losses)
plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Double DQN / Dueling DQN Training Loss")
```

é€™å¯ç”¨ä¾†æ¯”è¼ƒä¸åŒæ¶æ§‹åœ¨å­¸ç¿’éç¨‹ä¸­çš„ç©©å®šæ€§èˆ‡æ”¶æ–‚é€Ÿåº¦ã€‚

### Double DQNçµæœ

![1746443793100](image/HW4-2_Double_Dueling/1746443793100.png)

### Double DQNçµæœ

![1746443815494](image/HW4-2_Double_Dueling/1746443815494.png)
