# Homework 4: DQN and its variantsï¼ˆæ­£å¼ç‰ˆè©³ç´°ä¸­æ–‡å ±å‘Šï¼‰

---

# 1. ä½œæ¥­è¦æ±‚ç¢ºèª

æœ¬æ¬¡ä½œæ¥­å…±åˆ†ç‚ºä¸‰å¤§éƒ¨åˆ†ï¼š

1. âš™ï¸ HW4-1: ç°¡å–®ç’°å¢ƒä¸‹çš„åŸºæœ¬ DQN å¯¦ç¾
   - é‹è¡Œæä¾›çš„ç¨‹å¼ç¢¼æˆ–é«”é©—ç·©è¡å€
   - èˆ‡ ChatGPT è¨è«–ä»£ç¢¼ä»¥æ¾„æ¸…ç†è§£
   - æäº¤ä¸€ä»½ç°¡çŸ­çš„ç†è§£å ±å‘Š
   - å¯¦ä½œ Experience Replay Buffer

2. âš–ï¸ HW4-2: ç©å®¶æ¨¡å¼ä¸‹çš„å¢å¼· DQN è®Šé«”
   - å¯¦æ–½ä¸¦æ¯”è¼ƒ Double DQN èˆ‡ Dueling DQN
   - å¼·èª¿æ”¹é€²åŸºæœ¬ DQN æ–¹æ³•çš„æŠ€å·§

3. ğŸ” HW4-3: éš¨æ©Ÿæ¨¡å¼ä¸‹å¢å¼· DQNï¼Œä¸¦åŠ å…¥è¨“ç·´æŠ€å·§
   - å°‡ DQN è½‰æ›ç‚º keras æˆ– PyTorch Lightning
   - æ•´åˆè¨“ç·´æŠ€è¡“ï¼ˆå¦‚æ¢¯åº¦å‰Šæ¸›ã€å­¸ç¿’ç‡èª¿åº¦ç­‰ï¼‰ä»¥æ”¹å–„ç©©å®šæ€§
   - å¯¦ä½œåŠ åˆ†é …ç›®ï¼ˆç©©å®šæŠ€å·§ï¼‰

---

# 2. HW4-1ï¼šNaive DQN for Static Mode

## èƒŒæ™¯èˆ‡ç›®æ¨™
ç›®æ¨™æ˜¯å¯¦ä½œæœ€åŸºç¤çš„ Deep Q-Learning (DQN)ï¼Œåœ¨ç©å®¶èˆ‡ç›®æ¨™ä½ç½®å›ºå®šçš„ç°¡å–®ç’°å¢ƒä¸­å­¸ç¿’æœ€ä½³ç­–ç•¥ã€‚è—‰æ­¤ç†Ÿæ‚‰ Experience Replayã€Target Network å’Œ epsilon-greedy ç­–ç•¥ã€‚

## è¨“ç·´æµç¨‹
1. åˆå§‹åŒ– Static Mode ç’°å¢ƒ
2. å»ºç«‹ç°¡å–® 2-layer Fully Connected çš„ DQN ç¶²è·¯
3. è¨­å®š Replay Bufferã€optimizerã€loss function
4. æ¯å›åˆä¾æ“š epsilon-greedy ç­–ç•¥é¸æ“‡è¡Œå‹•
5. å„²å­˜ç¶“é©—é€² Replay Bufferï¼Œé–‹å§‹æ‰¹æ¬¡è¨“ç·´
6. å®šæœŸåŒæ­¥ target network
7. æ¯50å›åˆè¨˜éŒ„ reward

## æŠ€è¡“èªªæ˜
- **Experience Replay**ï¼šéš¨æ©Ÿå–æ¨£éå»ç¶“é©—ï¼Œæå‡å­¸ç¿’ç©©å®šæ€§
- **Target Network**ï¼šç©©å®šQå€¼ä¼°è¨ˆ
- **Epsilon Decay**ï¼šé€æ­¥æ¸›å°‘æ¢ç´¢ï¼Œå¢å¼· exploitation

### é‡è¦ç¨‹å¼ç¢¼ç‰‡æ®µ
```python
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)
```

## è¨“ç·´çµæœèˆ‡æ›²ç·š
![](1edbe2c9-7eff-44cf-81fc-8f447f7cfe31.png)

- èµ·åˆ reward æ³¢å‹•å¤§
- éš¨è¨“ç·´æ¬¡æ•¸å¢åŠ ï¼Œreward è¶¨æ–¼ä¸Šå‡
- æœ€å¾Œ reward ç©©å®šé”æˆç›®æ¨™

## é‡åˆ°çš„å•é¡Œèˆ‡ä¿®æ­£
- åˆæœŸ epsilon decay å¤ªå¿« â†’ èª¿æ•´ decay é€Ÿåº¦
- Replay Buffer å¤ªå°å°è‡´ overfitting â†’ å¢åŠ  buffer size

## å°çµèˆ‡æ”¹é€²å»ºè­°
- è‹¥ç’°å¢ƒæ›´è¤‡é›œï¼Œå¯å¼•å…¥ Double DQN é˜²æ­¢ Q-value éé«˜ä¼°è¨ˆ
- å¯å˜—è©¦åŠ å…¥ reward clipping ç©©å®šå­¸ç¿’

---

# 3. HW4-2ï¼šEnhanced DQN Variants for Player Mode

## èƒŒæ™¯èˆ‡ç›®æ¨™
éš¨æ©Ÿåˆå§‹ä½ç½®çš„ç©å®¶ä½¿å¾—ä»»å‹™æ›´å…·æŒ‘æˆ°æ€§ã€‚ç‚ºæ­¤éœ€è¦æ›´ç©©å®šä¸”ç²¾ç¢ºçš„ DQN æ”¹è‰¯ç‰ˆã€‚

## è¨“ç·´æµç¨‹
1. åˆå§‹åŒ– Player Mode ç’°å¢ƒ
2. å»ºç«‹ Double DQN èˆ‡ Dueling DQN
3. æ¯å›åˆæ ¹æ“š epsilon-greedy é¸æ“‡è¡Œå‹•
4. å„²å­˜ä¸¦è¨“ç·´ç¶“é©—
5. æ¯Nå›åˆåŒæ­¥ target network
6. åˆ†åˆ¥æ”¶é›† Double / Dueling çš„ reward æ›²ç·š

## æŠ€è¡“èªªæ˜
- **Double DQN**ï¼šå°‡ action selection èˆ‡ target evaluation åˆ†é–‹ï¼Œé¿å…éé«˜ä¼°è¨ˆ
- **Dueling DQN**ï¼šåˆ†é–‹ä¼°è¨ˆç‹€æ…‹åƒ¹å€¼èˆ‡å‹•ä½œå„ªå‹¢ï¼ŒåŠ é€Ÿå­¸ç¿’

### é‡è¦ç¨‹å¼ç¢¼ç‰‡æ®µ
```python
# Double DQN Target
next_actions = policy_net(next_states).argmax(1)
next_q_values = target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
```

```python
# Dueling DQN Forward
def forward(self, x):
    x = torch.relu(self.fc1(x))
    x = torch.relu(self.fc2(x))
    value = self.value(x)
    advantage = self.advantage(x)
    return value + (advantage - advantage.mean(dim=1, keepdim=True))
```

## è¨“ç·´çµæœæ¯”è¼ƒèˆ‡æ›²ç·š
![](d80e846c-76c2-4bf3-8716-0911fdb29db5.png)

- Double DQN è¡¨ç¾ç©©å®šï¼ŒDueling DQN åˆæœŸæ”¶æ–‚é€Ÿåº¦è¼ƒå¿«
- Dueling Q-learning åœ¨ early exploration ç‰¹åˆ¥æœ‰æ•ˆ

## é‡åˆ°çš„å•é¡Œèˆ‡ä¿®æ­£
- DuelingåˆæœŸéåº¦ä¼°è¨ˆå•é¡Œ â†’ å¢åŠ  buffer æ›´æ–°é »ç‡
- Doubleåœ¨ç¨€ç–çå‹µæ™‚æ”¶æ–‚æ…¢ â†’ å»¶é•· epsilon decay æ™‚é–“

## å°çµèˆ‡æ”¹é€²å»ºè­°
- å¯å˜—è©¦çµåˆ Double + Dueling
- å¯ä»¥åŠ ä¸Š Prioritized Replay æˆ– Multi-Step TD æ”¹é€²å–æ¨£èˆ‡æ›´æ–°æ–¹å¼

---

# 4. HW4-3ï¼šEnhance DQN for Random Mode with Training Tips

## èƒŒæ™¯èˆ‡ç›®æ¨™
éš¨æ©Ÿç’°å¢ƒè®Šæ•¸å¢åŠ ï¼Œè¦æ±‚æ¨¡å‹èƒ½åœ¨å¤§ç¯„åœæƒ…å¢ƒä¸‹ç©©å®šå­¸ç¿’ã€‚å¼•å…¥è¨“ç·´æŠ€è¡“ä¾†ç©©å®šèˆ‡æå‡æ•ˆæœã€‚

## è¨“ç·´æµç¨‹
1. è½‰æ› DQN æˆ PyTorch Lightning æ¶æ§‹
2. å¯¦ä½œ Double DQN èˆ‡ Dueling DQN Lightning ç‰ˆæœ¬
3. åŠ å…¥ Gradient Clippingã€Learning Rate Scheduler
4. æ¯å›åˆè¨˜éŒ„ rewardï¼Œåˆ†ææ›²ç·šè¡¨ç¾

## æŠ€è¡“èªªæ˜
- **PyTorch Lightning**ï¼šæ¨¡çµ„åŒ–è¨“ç·´ï¼Œæé«˜æ˜“è®€æ€§èˆ‡å¯æ“´å±•æ€§
- **Gradient Clipping**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **Learning Rate Scheduler**ï¼šå‹•æ…‹èª¿æ•´å­¸ç¿’ç‡ï¼Œä¿ƒé€²æ”¶æ–‚

### é‡è¦ç¨‹å¼ç¢¼ç‰‡æ®µ
```python
self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)
self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)
loss.backward()
torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
self.optimizer.step()
```

## è¨“ç·´çµæœæ¯”è¼ƒèˆ‡æ›²ç·š
![](50ad5473-380e-4fa5-9477-36242673afe3.png)

- Double DQN åœ¨ high-variance éš¨æ©Ÿç’°å¢ƒä¸­è¡¨ç¾è¼ƒç©©
- Dueling DQN early learning ç¨å¾®æ›´å¿«ï¼Œä½†ä¸­æœŸéœ‡ç›ªè¼ƒå¤§

## é‡åˆ°çš„å•é¡Œèˆ‡ä¿®æ­£
- LightningåˆæœŸ optimizer éŒ¯èª¤ â†’ æ”¹ç”¨æ‰‹å‹•ç®¡ç† optimizer
- Random modeæ”¶æ–‚æ…¢ â†’ æ¸›æ…¢ epsilon decay é€Ÿåº¦

## å°çµèˆ‡æ”¹é€²å»ºè­°
- éš¨æ©Ÿç’°å¢ƒä¸‹ä½¿ç”¨ Lightning+ç©©å®šæŠ€è¡“æ•ˆæœæ˜é¡¯
- æœªä¾†å¯ä»¥å¼•å…¥ï¼š
  - Distributional DQN
  - Rainbow DQN ç¶œåˆå‹æ”¹è‰¯
  - NoisyNet å¢å¼· exploration

---
