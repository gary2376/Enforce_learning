{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0f35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 載入資料與基本處理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tslearn.metrics import cdist_dtw\n",
    "from sklearn.metrics import silhouette_score\n",
    "from stockstats import StockDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append(\"/Users/gary/Documents/project/RL/code/FinRL\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97eaf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 讀入所有股票資料\n",
    "def load_all_stock_data(folder_path):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            path = os.path.join(folder_path, filename)\n",
    "            tic = filename.replace(\"converted_\", \"\").replace(\".csv\", \"\")\n",
    "            df = pd.read_csv(path)\n",
    "            df['tic'] = tic\n",
    "            all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "folder_path = \"/Users/gary/Documents/project/RL/code/converted_stock\"\n",
    "raw_df = load_all_stock_data(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef7b6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 log return 計算與標準化\n",
    "def preprocess_stock_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    full_dates = pd.date_range(df['date'].min(), df['date'].max())\n",
    "\n",
    "    processed = {}\n",
    "    for tic in df['tic'].unique():\n",
    "        sub_df = df[df['tic'] == tic].copy()\n",
    "        sub_df = sub_df.set_index('date').reindex(full_dates)\n",
    "        sub_df['close'] = sub_df['close'].interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
    "        sub_df['log_return'] = np.log(sub_df['close'] / sub_df['close'].shift(1))\n",
    "        sub_df = sub_df[['log_return']].dropna()\n",
    "        processed[tic] = sub_df['log_return']\n",
    "    \n",
    "    result_df = pd.DataFrame(processed)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = pd.DataFrame(scaler.fit_transform(result_df), columns=result_df.columns, index=result_df.index)\n",
    "    \n",
    "    return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222c3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import cdist_dtw\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "import numpy as np\n",
    "\n",
    "def dtw_clustering_pyclustering(log_return_df, k=3):\n",
    "    series_array = log_return_df.T.values[..., np.newaxis]\n",
    "    distance_matrix = cdist_dtw(series_array)\n",
    "\n",
    "    # 初始中心點隨便選 k 個 index\n",
    "    initial_medoids = list(range(k))\n",
    "\n",
    "    kmedoids_instance = kmedoids(distance_matrix, initial_medoids, data_type='distance_matrix', ccore=False)\n",
    "    kmedoids_instance.process()\n",
    "    clusters = kmedoids_instance.get_clusters()\n",
    "\n",
    "    label_map = {}\n",
    "    stock_list = list(log_return_df.columns)\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for idx in cluster:\n",
    "            label_map[stock_list[idx]] = i\n",
    "    return label_map, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a63269db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧮 技術指標計算與資料分割\n",
    "from stockstats import StockDataFrame\n",
    "\n",
    "def add_technical_indicators(df, indicators=[\"macd\", \"rsi_30\", \"cci_30\", \"wr_14\"]):\n",
    "    df_list = []\n",
    "    for tic in df['tic'].unique():\n",
    "        sub_df = df[df['tic'] == tic].copy()\n",
    "        sub_df = sub_df.sort_values(\"date\")  # ⚠️ 要先有 'date' 欄位才能排序\n",
    "\n",
    "        # StockDataFrame 會把 date 移成 index\n",
    "        stock = StockDataFrame.retype(sub_df)\n",
    "\n",
    "        for indicator in indicators:\n",
    "            try:\n",
    "                sub_df[indicator] = stock[indicator]\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 無法計算指標 {indicator}: {e}\")\n",
    "\n",
    "        # ✅ 關鍵：確保 'date' 是欄位（不是 index）\n",
    "        if 'date' not in sub_df.columns:\n",
    "            sub_df['date'] = sub_df.index\n",
    "\n",
    "        df_list.append(sub_df)\n",
    "\n",
    "    result = pd.concat(df_list).reset_index(drop=True)  # 避免後續混淆 index\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_train_test(df, split_date=\"2024-01-01\"):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    train = df[df['date'] < split_date]\n",
    "    test = df[df['date'] >= split_date]\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a11ae363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ 建立訓練環境與模型訓練\n",
    "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "def create_env_for_stock_np(df, stock_tic, indicators, initial_amount=1e6, if_train=True):\n",
    "    import numpy as np\n",
    "    from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
    "\n",
    "    df = df[df['tic'] == stock_tic].copy().dropna().reset_index(drop=True)\n",
    "\n",
    "    price_array = df[['close']].values\n",
    "    tech_array = df[indicators].values\n",
    "    turbulence_array = np.zeros(len(df))  # 若無 turbulence，就用全 0\n",
    "\n",
    "    env = StockTradingEnv(\n",
    "        config={\n",
    "            \"price_array\": price_array,\n",
    "            \"tech_array\": tech_array,\n",
    "            \"turbulence_array\": turbulence_array,\n",
    "            \"if_add_price\": True,\n",
    "            \"if_add_tech\": True,\n",
    "            \"if_add_turbulence\": False,\n",
    "            \"risk_indicator_col\": \"turbulence\",\n",
    "            \"initial_amount\": initial_amount,\n",
    "            \"buy_cost_pct\": 0.001,\n",
    "            \"sell_cost_pct\": 0.001,\n",
    "            \"reward_scaling\": 1e-4,\n",
    "            \"if_train\": if_train  # ✅ 這一行是你之前漏掉的\n",
    "        }\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "def train_ppo_model(env, model_path):\n",
    "    agent = DRLAgent(env=env)\n",
    "    model = agent.get_model(\"ppo\")\n",
    "    trained_model = agent.train_model(model=model, tb_log_name=\"ppo\", total_timesteps=300000)\n",
    "    trained_model.save(model_path)\n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc22673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trained_model, env, initial_amount):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_rewards = []\n",
    "    portfolio_values = [state[0]]\n",
    "\n",
    "    step_count = 0\n",
    "    while not done:\n",
    "        action, _ = trained_model.predict(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_rewards.append(reward)\n",
    "        portfolio_values.append(state[0])\n",
    "        step_count += 1\n",
    "\n",
    "    print(f\"✅ 模型測試完成，共執行 {step_count} 步。\")\n",
    "    print(f\"📈 總 reward 長度：{len(total_rewards)}\")\n",
    "    print(f\"💰 Portfolio value 長度：{len(portfolio_values)}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.cumsum(total_rewards))\n",
    "    plt.title(\"累積 Reward\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(portfolio_values)\n",
    "    plt.title(\"Portfolio Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"result_plot.png\")  # ✅ 存圖備用\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d21d6549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 訓練第 0 群：股票 1722.TW\n",
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| rollout/           |              |\n",
      "|    ep_len_mean     | 726          |\n",
      "|    ep_rew_mean     | 8.74         |\n",
      "| time/              |              |\n",
      "|    fps             | 6271         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 0            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.090232246 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 726          |\n",
      "|    ep_rew_mean          | 16.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4418         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045045917 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0131       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.463        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    reward               | -0.15495901  |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 1.81         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 726         |\n",
      "|    ep_rew_mean          | 24.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4070        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003769112 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.025      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.58        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    reward               | 0.26787078  |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 8.23        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 726         |\n",
      "|    ep_rew_mean          | 23.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3876        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001912985 |\n",
      "|    clip_fraction        | 0.00889     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.5         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00106    |\n",
      "|    reward               | -0.16151358 |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 7.4         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 726          |\n",
      "|    ep_rew_mean          | 32.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3801         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058846986 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.15         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00362     |\n",
      "|    reward               | 0.32459477   |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 4.49         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 726         |\n",
      "|    ep_rew_mean          | 40.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3734        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004682968 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    reward               | 3.355957    |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 32.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 726          |\n",
      "|    ep_rew_mean          | 50.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3694         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023023016 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.145        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 31.3         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00098     |\n",
      "|    reward               | 1.0876938    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 54.7         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 726        |\n",
      "|    ep_rew_mean          | 61.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3663       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00299195 |\n",
      "|    clip_fraction        | 0.00806    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.286      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 36.3       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.00302   |\n",
      "|    reward               | 0.96401244 |\n",
      "|    std                  | 0.998      |\n",
      "|    value_loss           | 78.3       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m env \u001b[38;5;241m=\u001b[39m create_env_for_stock_np(train_data, stock_tic\u001b[38;5;241m=\u001b[39mrepresentative_tic, indicators\u001b[38;5;241m=\u001b[39mindicators, if_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ppo_cluster_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 38\u001b[0m, in \u001b[0;36mtrain_ppo_model\u001b[0;34m(env, model_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m agent \u001b[38;5;241m=\u001b[39m DRLAgent(env\u001b[38;5;241m=\u001b[39menv)\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m trained_model\u001b[38;5;241m.\u001b[39msave(model_path)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_model\n",
      "File \u001b[0;32m~/Documents/project/water/water_venv/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py:117\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[0;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m    115\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m    116\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/project/water/water_venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/water/water_venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/project/water/water_venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    277\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/Documents/project/water/water_venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/water/water_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/project/water/water_venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "indicators = [\"macd\", \"rsi_30\", \"cci_30\", \"wr_14\"]\n",
    "raw_df = add_technical_indicators(raw_df, indicators=indicators)\n",
    "log_return_df = preprocess_stock_data(raw_df)\n",
    "cluster_labels, best_k = dtw_clustering_pyclustering(log_return_df, k=5)\n",
    "\n",
    "train_data, test_data = split_train_test(raw_df)\n",
    "\n",
    "model_dir = \"/Users/gary/Documents/project/RL/code/model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ✅ 訓練每一群的代表模型\n",
    "for group_id in sorted(set(cluster_labels.values())):\n",
    "    group_stocks = [tic for tic, g in cluster_labels.items() if g == group_id]\n",
    "    representative_tic = group_stocks[0]\n",
    "    print(f\"🧠 訓練第 {group_id} 群：股票 {representative_tic}\")\n",
    "\n",
    "    env = create_env_for_stock_np(train_data, stock_tic=representative_tic, indicators=indicators, if_train=True)\n",
    "    model_path = f\"{model_dir}/ppo_cluster_{group_id}.zip\"\n",
    "    \n",
    "    model = train_ppo_model(env, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10eeaf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "✅ 模型測試完成，共執行 97 步。\n",
      "📈 總 reward 長度：97\n",
      "💰 Portfolio value 長度：98\n"
     ]
    }
   ],
   "source": [
    "# 🧪 測試區塊：載入已訓練模型並進行回測與視覺化\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 📍 參數重用\n",
    "indicators = [\"macd\", \"rsi_30\", \"cci_30\", \"wr_14\"]\n",
    "target_tic = \"1402.TW\"\n",
    "initial_amount = 1e6\n",
    "\n",
    "# 📁 模型載入\n",
    "model_dir = \"/Users/gary/Documents/project/RL/code/model\"\n",
    "target_group = cluster_labels[target_tic]  # ← 若這是新檔案，請重新載入 cluster_labels\n",
    "model_path = f\"{model_dir}/ppo_cluster_{target_group}.zip\"\n",
    "\n",
    "# ✅ 建立測試環境\n",
    "test_env = create_env_for_stock_np(test_data, stock_tic=target_tic, indicators=indicators, if_train=False)\n",
    "\n",
    "# 📦 載入模型 & 評估\n",
    "model = PPO.load(model_path, env=test_env)\n",
    "evaluate_model(model, test_env, initial_amount=initial_amount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84c4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
